from app.state.agent_state import AgentState
from app.tools.visual_tools import process_image_for_search, detect_product_from_image
from app.tools.vector_tools import search_visual_products, search_text_products
from app.services.vector_service import vector_service
from app.utils.config import settings
import logging
import uuid
from langchain_core.messages import AIMessage

logger = logging.getLogger(__name__)

async def visual_search_agent_node(state: AgentState):
    """
    Visual Search Agent: Processes product images using TRI-Strategy.
    
    Strategy:
        1. Visual Similarity (DINOv2) -> Finds visually similar items in DB.
        2. OCR Text Hunt (Llama Vision) -> Reads text on package -> Searches Text DB.
        3. Semantic Description (Llama Vision) -> Describes image -> Searches Text DB.
    
    Args:
        state: Agent state containing image_url
        
    Returns:
        Updated state with 'visual_matches' string combining all findings.
    """
    print(f"\n>>> VISUAL AGENT (Advanced): Processing image search")
    logger.info("Visual Search Agent: Starting advanced image analysis")
    
    messages = state["messages"]
    last_message = messages[-1]
    image_url = state.get("image_url") or last_message.additional_kwargs.get("image_url")
    
    if not image_url:
        return {
            "messages": [AIMessage(content="Please send an image to search for products.")],
            "error": "no_image"
        }
    
    try:
        # A. Run Detection (Llama Vision)
        detection_data = await detect_product_from_image.ainvoke(image_url)
        
        detected_text = detection_data.get("detected_text", "")
        visual_desc = detection_data.get("visual_description", "")
        product_type = detection_data.get("product_type", "")
        confidence = detection_data.get("confidence", 0.0)
        
        logger.info(f"Image Analysis: Text='{detected_text}', Type='{product_type}'")
        
        # B. Strategy 1: Visual Similarity (DINOv2)
        visual_matches = ""
        embedding = None
        try:
            embedding = await process_image_for_search.ainvoke(image_url)
            if embedding:
                visual_matches = await search_visual_products.ainvoke(embedding)
        except Exception as ve:
            logger.warning(f"Visual strategy failed: {ve}")

        # C. Strategy 2: OCR Text Matches
        ocr_matches = ""
        if detected_text and len(detected_text) > 3: # Ignore short noise
            logger.info(f"Visual Search: Text detected '{detected_text}'")
            ocr_matches = await search_text_products.ainvoke(detected_text)
            if "No products found" in ocr_matches:
                ocr_matches = "" # Clear if noise
            else:
                ocr_matches = f"MATCHES FROM PACKAGE TEXT ('{detected_text}'):\n{ocr_matches}"

        # D. Strategy 3: Semantic Description Search (Fallback)
        # Use the description generated by detect_product_from_image
        semantic_matches = ""
        if visual_desc:
            search_query = f"{product_type} {visual_desc}"
            semantic_matches = await search_text_products.ainvoke(search_query)

        # Combine
        combined_results = f"""
Image Analysis Result:
- Detected Text: {detected_text or 'None'}
- Type: {product_type}
- Visuals: {visual_desc}

{ocr_matches}

{visual_matches if visual_matches else '(No direct visual look-alikes found)'}

Semantic Matches (based on description):
{semantic_matches}
"""
        
        # Log Auto-Enrichment Opportunity
        if confidence > 0.95 and detected_text and not visual_matches:
             # Verify against DB to get canonical Name for the vector metadata
             try:
                 from sentence_transformers import SentenceTransformer
                 model = SentenceTransformer('all-MiniLM-L6-v2')
                 text_vec = model.encode(detected_text).tolist()
                 
                 # Check Pinecone for Best Text Match
                 text_check = await vector_service.query_vectors(
                    index_name=settings.PINECONE_INDEX_PRODUCTS_TEXT,
                    vector=text_vec,
                    top_k=1
                 )
                 
                 if text_check and text_check[0]['score'] > 0.85:
                     matched_name = text_check[0]['metadata'].get('name')
                     logger.info(f"ðŸ’¡ AUTO-ENRICHMENT: Auto-learning image for '{matched_name}' (OCR: {detected_text})")
                     
                     # UPSERT Logic
                     if embedding:
                         record_id = f"user_gen_{uuid.uuid4().hex[:8]}"
                         metadata = {
                             "name": matched_name, # Use CANONICAL name
                             "source": "user_enriched",
                             "original_ocr": detected_text,
                             "confidence": confidence,
                             "description": visual_desc
                             # Note: We aren't saving the image file to disk here to keep it serverless/clean
                             # relying on the generic visual embedding.
                             # If we needed to show this image later, we'd need to save 'image_url' 
                             # but user URLs expire! For search-only, embedding is enough.
                         }
                         
                         vector_service.upsert_vectors(
                             index_name=settings.PINECONE_INDEX_PRODUCTS,
                             vectors=[{
                                 "id": record_id,
                                 "values": embedding,
                                 "metadata": metadata
                             }]
                         )
                         logger.info(f"âœ… LEARNED: Saved visual vector for {matched_name}")
                         
                         # Add a note to the result so the agent knows it learned something
                         combined_results += f"\n[System Note]: I just learned what '{matched_name}' looks like from this image!"
                 
             except Exception as enrich_err:
                 logger.error(f"Enrichment failed: {enrich_err}")

        return {
            "visual_matches": combined_results,
            "query_type": "image"
        }
        
    except Exception as e:
        logger.error(f"Visual Search Error: {e}", exc_info=True)
        return {
            "messages": [AIMessage(content="I had trouble seeing that image clearly. Could you type the product name?")],
            "error": str(e)
        }
